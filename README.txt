Here you can find a project (bank_marketing.ipynb) where I used Single Decision Classifier, Random Forest Classifier, Ada Boost Classifier, Gradient Boosting Classifier, K-Nearest Neighbors to predict if the client subscribed a term deposit. In the file bank-names.txt there is an information about all attributes. I created a few plots to visualize the data and show relationships between some attributes. I used Grid Search CV to find the best hyperparameters for models

I got the best performance with the Gradient Boosting Classifier. It is easy to see that trees methods performed better than K Nearest Neighbors. The results indicate a significant class imbalance, with the "no" class having much higher precision, recall, and F1-score compared to the "yes" class. This suggests that the model performs well with the majority class but struggles with the minority class. A way to improve the results can be to experiment with hyperparameters, adjust parameters like n_estimators, max_depth, and min_samples_split to find the optimal configuration, use techniques like oversampling (increasing the number of instances of the minority class) or undersampling (decreasing the number of instances of the majority class) to balance the class distribution, or try to use different types of models.
